{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model minimizes the error of the model or maximizes the likelihood of the data\n",
    "# It will represent the solution to some sort of optimization problem\n",
    "\n",
    "# One approach to solving optimization problems is called the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea Behind Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradient is the vector of partial derivatives.  This gives the input direction in which the function most\n",
    "# quickly increases\n",
    "# Pick a random starting point, compute the gradient, take a small step in the direction of the gradient\n",
    "# function increase most = maximization, function decrease = minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If f is a function of one variabl, its derivative at a point x measures how f(x) changes when we make a small\n",
    "# change to x (The limit of the difference quotients)\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h # as h approaches 0\n",
    "def square(x):\n",
    "    return x * x\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "from functools import partial\n",
    "\n",
    "# Derivative is the slope of the tangent line at (x, f(x))\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.0001)\n",
    "# plot to show they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10, 10)\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "#plt.plot(x, map(derivative, x), 'rx', label='Actual')\n",
    "#plt.plot(x, map(derivative_estimate, x), 'b+', label='Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the ith partial derivative by tearting it as a function of just the ith variable, holding others fixed\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)     # add h to just the ith element of v\n",
    "        for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(x) - f(v)) / h\n",
    "\n",
    "# Estimate using difference quotients is computationally expensive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-feb7b50171cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_squares_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# compute the gradient at v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnext_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# take a negative gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m:\u001b[0m              \u001b[0;31m# stop if we're converging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_v\u001b[0m                                       \u001b[0;31m# continue if we're not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's use gradients to find the minimum among all 3-D vectors\n",
    "import random \n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v] \n",
    "# pick a random starting point\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradientRuntimeError: matplotlib does not support generators as input(v)            # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)                # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance:              # stop if we're converging\n",
    "        break\n",
    "    v = next_v                                       # continue if we're not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the gradient (and takes a step) for only one point at a time\n",
    "# Cycles over data repeatedly until it reaches a stopping point\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]       # create a list of indexes\n",
    "    random.shuffle(indexes)                         # shuffle them\n",
    "    for i in indexes:                               # return the data in that order\n",
    "        yield data[i]\n",
    "# Decrease the step size and eventually quit\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data = zip(x, y)                             # tupled data\n",
    "    theta = theta_0                              # initial guess\n",
    "    alpha = alpha_0                              # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")    # the min so far\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # if we've found a new min, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_o\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "        \n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                              negate_all(gradient_fn),\n",
    "                              x, y, theta_0, alpha_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
